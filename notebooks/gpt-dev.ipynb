{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following along https://colab.research.google.com/drive/1JMLa53HDuA-i7ZBmqV7ZnA3c_fvtXnx-?usp=sharing but with JAX/Flax"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 123,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/Cellar/python@3.12/3.12.7_1/Frameworks/Python.framework/Versions/3.12/lib/python3.12/pty.py:95: RuntimeWarning: os.fork() was called. os.fork() is incompatible with multithreaded code, and JAX is multithreaded, so this will likely lead to a deadlock.\n",
      "  pid, fd = os.forkpty()\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "mkdir: data: File exists\n",
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 1089k  100 1089k    0     0  2433k      0 --:--:-- --:--:-- --:--:-- 2436k\n"
     ]
    }
   ],
   "source": [
    "!mkdir data\n",
    "!curl https://raw.githubusercontent.com/karpathy/char-rnn/master/data/tinyshakespeare/input.txt -o data/tinyshakespeare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Corpus size: 1115394\n",
      "First Citizen:\n",
      "Before we proceed any further, hear me speak.\n",
      "\n",
      "All:\n",
      "Speak, speak.\n",
      "\n",
      "First Citizen:\n",
      "You are all resolved rather to die than to famish?\n",
      "\n",
      "All:\n",
      "Resolved. resolved.\n",
      "\n",
      "First Citizen:\n",
      "First, you know Caius Marcius is chief enemy to the people.\n",
      "\n",
      "All:\n",
      "We know't, we know't.\n",
      "\n",
      "First Citizen:\n",
      "Let us kill him, and we'll have corn at our own price.\n",
      "Is't a verdict?\n",
      "\n",
      "All:\n",
      "No more talking on't; let it be done: away, away!\n",
      "\n",
      "Second Citizen:\n",
      "One word, good citizens.\n",
      "\n",
      "First Citizen:\n",
      "We are accounted poor citizens, the patricians good.\n",
      "What authority surfeits on would relieve us: if they\n",
      "would yield us but the superfluity, while it were\n",
      "wholesome, we might guess they relieved us humanely;\n",
      "but they think we are too dear: the leanness that\n",
      "afflicts us, the object of our misery, is as an\n",
      "inventory to particularise their abundance; our\n",
      "sufferance is a gain to them Let us revenge this with\n",
      "our pikes, ere we become rakes: for the gods know I\n",
      "speak this in hunger for bread, not in thirst for revenge.\n",
      "\n",
      "\n",
      "Vocabulary size: 65\n",
      "\n",
      " !$&',-.3:;?ABCDEFGHIJKLMNOPQRSTUVWXYZabcdefghijklmnopqrstuvwxyz\n"
     ]
    }
   ],
   "source": [
    "with open('data/tinyshakespeare') as f:\n",
    "    text = f.read()\n",
    "\n",
    "print('Corpus size: ' + str(len(text)))\n",
    "print(text[:1000])\n",
    "\n",
    "vocab = list(set(text))\n",
    "vocab_size = len(vocab)\n",
    "print('Vocabulary size: ' + str(len(vocab)))\n",
    "print(''.join(sorted(vocab)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "hello world\n"
     ]
    }
   ],
   "source": [
    "itos = {i:s for i,s in enumerate(vocab)}\n",
    "stoi = {s:i for i,s in enumerate(vocab)}\n",
    "\n",
    "encode = lambda x: [stoi[s] for s in x]\n",
    "decode = lambda x: ''.join([itos[i] for i in x])\n",
    "\n",
    "print(decode(encode('hello world')))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 126,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "int32\n",
      "(1115394,)\n",
      "[47  5 64 18 57 45 13  5 57  5 12 53 40 61 54 55 53 38  4 64 53 45 27 53\n",
      " 45 50 64  4 62 53 53 14 45 42 40 23 45 38 43 64 57  6 53 64  3 45  6 53\n",
      " 42 64 45 41 53 45 18 50 53 42  9  0 54 54 29 24 24 61 54 49 50 53 42  9\n",
      "  3 45 18 50 53 42  9  0 54 54 47  5 64 18 57 45 13  5 57  5 12 53 40 61\n",
      " 54 48  4 43]\n"
     ]
    }
   ],
   "source": [
    "import jax.numpy as jnp\n",
    "\n",
    "data = jnp.array(encode(text), dtype=jnp.int32)\n",
    "print(data.dtype)\n",
    "print(data.shape)\n",
    "print(data[:100])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 127,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data = data[: int(.9 * len(data))]\n",
    "val_data = data[int(.9 * len(data)):]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 128,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(Array([[45, 41, 42, 40, 57, 24, 53, 54],\n",
      "       [16, 36, 56, 61, 54, 56,  3, 45],\n",
      "       [ 5, 41,  0, 54, 55, 43, 57, 45],\n",
      "       [45, 18, 43, 62,  6, 45, 24, 53]], dtype=int32), Array([[41, 42, 40, 57, 24, 53, 54,  4],\n",
      "       [36, 56, 61, 54, 56,  3, 45, 24],\n",
      "       [41,  0, 54, 55, 43, 57, 45,  6],\n",
      "       [18, 43, 62,  6, 45, 24, 53, 40]], dtype=int32))\n"
     ]
    }
   ],
   "source": [
    "import jax\n",
    "\n",
    "batch_size = 4\n",
    "block_size = 8\n",
    "\n",
    "dynamic_slice_vmap = jax.vmap(jax.lax.dynamic_slice, in_axes=(None, 0, None))\n",
    "\n",
    "\n",
    "@jax.jit\n",
    "def get_batch(data, key):\n",
    "    ix = jax.random.randint(key, shape=(batch_size, 1), minval=0, maxval=len(data) - block_size)\n",
    "    x = dynamic_slice_vmap(data, ix, (block_size,))\n",
    "    y = dynamic_slice_vmap(data, ix + 1, (block_size,))\n",
    "    return x, y\n",
    "\n",
    "\n",
    "key = jax.random.key(1337)\n",
    "print(get_batch(train_data, key))\n",
    "xb, yb = get_batch(train_data, key)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "\n",
    "class BigramLanguageModel(nnx.Module):\n",
    "    def __init__(self, vocab_size, rngs: nnx.Rngs):\n",
    "        self.rngs = rngs\n",
    "        self.token_embedding_table = nnx.Embed(num_embeddings=vocab_size, features=vocab_size, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        logits = self.token_embedding_table(x)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, x, length): # x has the shape (batch_size, block_size)\n",
    "        for i in range(length):\n",
    "            logits = self(x)\n",
    "            next_token = jax.random.categorical(self.rngs.next(), logits[:, -1])\n",
    "            x = jnp.concatenate([x, next_token[:, None]], axis=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "key = jax.random.key(1337)\n",
    "rngs = nnx.Rngs(key)\n",
    "model = BigramLanguageModel(vocab_size, rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 130,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "\n",
    "class Lookback(nnx.Module):\n",
    "    def __init__(self, rngs: nnx.Rngs):\n",
    "        self.rngs = rngs\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        tril = jnp.tril(jnp.ones((T, T)))\n",
    "        attn = jnp.zeros((T, T))\n",
    "        attn = jnp.where(tril == 0, float('-inf'), attn)\n",
    "        attn = jax.nn.softmax(attn)\n",
    "        return attn @ x\n",
    "\n",
    "class NgramLanguageModel(nnx.Module):\n",
    "    def __init__(self, vocab_size, n_embed, rngs: nnx.Rngs):\n",
    "        self.rngs = rngs\n",
    "        self.token_embedding_table = nnx.Embed(num_embeddings=vocab_size, features=n_embed, rngs=rngs)\n",
    "        self.position_embedding_table = nnx.Embed(num_embeddings=block_size, features=n_embed, rngs=rngs)\n",
    "        self.lookback = Lookback(rngs)\n",
    "        self.lm_head = nnx.Linear(n_embed, vocab_size, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T = x.shape\n",
    "        x = self.token_embedding_table(x) + self.position_embedding_table(jnp.arange(T))\n",
    "        x = self.lookback(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, x, length):\n",
    "        for i in range(length):\n",
    "            logits = self(x[:, -block_size:])\n",
    "            next_token = jax.random.categorical(self.rngs.next(), logits[:, -1])\n",
    "            x = jnp.concatenate([x, next_token[:, None]], axis=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "key = jax.random.key(1337)\n",
    "rngs = nnx.Rngs(key)\n",
    "model = NgramLanguageModel(vocab_size, 32, rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 131,
   "metadata": {},
   "outputs": [],
   "source": [
    "from flax import nnx\n",
    "import optax\n",
    "\n",
    "class SelfAttentionHead(nnx.Module):\n",
    "    def __init__(self, n_embed, head_dim, rngs: nnx.Rngs):\n",
    "        self.rngs = rngs\n",
    "        self.query = nnx.Linear(n_embed, head_dim, rngs=rngs, use_bias=False)\n",
    "        self.key = nnx.Linear(n_embed, head_dim, rngs=rngs, use_bias=False)\n",
    "        self.value = nnx.Linear(n_embed, head_dim, rngs=rngs, use_bias=False)\n",
    "        self.tril = jnp.tril(jnp.ones((block_size, block_size)))\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        B, T, C = x.shape\n",
    "        q = self.query(x)\n",
    "        k = self.key(x)\n",
    "\n",
    "        attn = jnp.einsum('btd,bTd->btT', q, k) / jnp.sqrt(16)\n",
    "\n",
    "        attn = jnp.where(self.tril[:T, :T] == 0, float('-inf'), attn)\n",
    "        attn = jax.nn.softmax(attn)\n",
    "\n",
    "        v = self.value(x)\n",
    "        return attn @ v\n",
    "    \n",
    "class MultiHeadAttention(nnx.Module):\n",
    "    def __init__(self, n_embed, n_head, head_dim, rngs: nnx.Rngs):\n",
    "        self.rngs = rngs\n",
    "        self.heads = [SelfAttentionHead(n_embed, head_dim, rngs) for _ in range(n_head)]\n",
    "        self.proj = nnx.Linear(n_embed, n_embed, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = jnp.concatenate([head(x) for head in self.heads], axis=-1)\n",
    "        return self.proj(x)\n",
    "    \n",
    "class FeedForward(nnx.Module):\n",
    "    def __init__(self, n_embed, rngs: nnx.Rngs):\n",
    "        self.rngs = rngs\n",
    "        self.fc1 = nnx.Linear(n_embed, 4 * n_embed, rngs=rngs)\n",
    "        self.fc2 = nnx.Linear(4 * n_embed, n_embed, rngs=rngs)\n",
    "    \n",
    "    def __call__(self, x):\n",
    "        return self.fc2(jax.nn.relu(self.fc1(x)))\n",
    "\n",
    "class Block(nnx.Module):\n",
    "    def __init__(self, n_embed, n_head, rngs: nnx.Rngs):\n",
    "        self.rngs = rngs\n",
    "        self.sa_heads = MultiHeadAttention(n_embed, n_head, n_embed // n_head, rngs=rngs)\n",
    "        self.ffwd = FeedForward(n_embed, rngs=rngs)\n",
    "        self.ln1 = nnx.LayerNorm(n_embed, rngs=rngs)\n",
    "        self.ln2 = nnx.LayerNorm(n_embed, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        x = x + self.sa_heads(self.ln1(x))\n",
    "        x = x + self.ffwd(self.ln2(x))\n",
    "        return x\n",
    "\n",
    "class GPT(nnx.Module):\n",
    "    def __init__(self, vocab_size, n_embed, n_head, n_blocks, rngs: nnx.Rngs):\n",
    "        self.rngs = rngs\n",
    "        self.token_embedding_table = nnx.Embed(num_embeddings=vocab_size, features=n_embed, rngs=rngs)\n",
    "        self.position_embedding_table = nnx.Embed(num_embeddings=block_size, features=n_embed, rngs=rngs)\n",
    "        self.blocks = nnx.Sequential(*[Block(n_embed, n_head, rngs) for _ in range(n_blocks)])\n",
    "        self.lm_head = nnx.Linear(n_embed, vocab_size, rngs=rngs)\n",
    "\n",
    "    def __call__(self, x):\n",
    "        B, T = x.shape\n",
    "        x = self.token_embedding_table(x) + self.position_embedding_table(jnp.arange(T))\n",
    "        x = self.blocks(x)\n",
    "        logits = self.lm_head(x)\n",
    "        return logits\n",
    "    \n",
    "    def generate(self, x, length):\n",
    "        for i in range(length):\n",
    "            logits = self(x[:, -block_size:])\n",
    "            next_token = jax.random.categorical(self.rngs.next(), logits[:, -1])\n",
    "            x = jnp.concatenate([x, next_token[:, None]], axis=1)\n",
    "        return x\n",
    "    \n",
    "\n",
    "key = jax.random.key(1337)\n",
    "rngs = nnx.Rngs(key)\n",
    "model = GPT(vocab_size, 32, 4, 4, rngs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 132,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "7.938052\n"
     ]
    }
   ],
   "source": [
    "def loss(model, x, targets):\n",
    "        logits = model(x)\n",
    "        return optax.softmax_cross_entropy_with_integer_labels(logits, targets).mean()\n",
    "\n",
    "xb, yb = get_batch(train_data, key)\n",
    "print(loss(model, xb, yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 133,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 10000/10000 [09:39<00:00, 17.27it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2.1941688\n",
      "2.198675\n"
     ]
    }
   ],
   "source": [
    "batch_size = 32\n",
    "\n",
    "import tqdm\n",
    "\n",
    "@nnx.jit\n",
    "def train_step(model, optimizer, xb, yb):\n",
    "    grads = (nnx.grad(loss))(model, xb, yb)\n",
    "    optimizer.update(grads)\n",
    "\n",
    "def train(key, model):\n",
    "    optimizer = nnx.Optimizer(model, optax.adam(1e-3))\n",
    "    for i in tqdm.trange(10000):\n",
    "        key, subkey = jax.random.split(key)\n",
    "        xb, yb = get_batch(train_data, subkey)\n",
    "        train_step(model, optimizer, xb, yb)\n",
    "train(key, model)\n",
    "print(loss(model, xb, yb))\n",
    "\n",
    "val_xb, val_yb = get_batch(val_data, key)\n",
    "print(loss(model, val_xb, val_yb))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      ".\n",
      "\n",
      "Come anly, thenee to tpastrusions for, low rentre, thrubly by himge farsfilf uny you, nooul and my face\n",
      "If shown him thy, bee ace thish han himfurs maqieso;\n",
      "Where wardly,\n",
      "Your thy lears lale my arde artcigh moow\n",
      "thisstalk'd whatys, wheld om-by brood of on it frumptlath caugh thow ims courtrn the stek but armxtell's Atcinls?\n",
      "\n",
      "SHUSTEN Rhe youd could\n",
      "The methy fand-do! our athed heach; all jowour shall, harthen:\n",
      "If our infircen Mencir,\n",
      "Whave the shatilted\n",
      "Ming; brelr have ongterwith bewsore thred\n"
     ]
    }
   ],
   "source": [
    "print([decode(row.tolist()) for row in model.generate(jnp.zeros((1, 1), dtype=jnp.int32), 500)][0])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
