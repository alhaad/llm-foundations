{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Following the course at https://github.com/rwitten/HighPerfLLMs2024"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Hyperparameters.\n",
    "seed = 1337\n",
    "batch_size = 8\n",
    "context_length = 128"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tensorflow_datasets as tfds\n",
    "ds = tfds.load('lm1b', split='train', shuffle_files=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "sentencepiece_trainer.cc(78) LOG(INFO) Starts training with : \n",
      "trainer_spec {\n",
      "  input_format: \n",
      "  model_prefix: data/lm1b_tokenizer\n",
      "  model_type: BPE\n",
      "  vocab_size: 1024\n",
      "  self_test_sample_size: 0\n",
      "  character_coverage: 0.9995\n",
      "  input_sentence_size: 0\n",
      "  shuffle_input_sentence: 1\n",
      "  seed_sentencepiece_size: 1000000\n",
      "  shrinking_factor: 0.75\n",
      "  max_sentence_length: 4192\n",
      "  num_threads: 16\n",
      "  num_sub_iterations: 2\n",
      "  max_sentencepiece_length: 16\n",
      "  split_by_unicode_script: 1\n",
      "  split_by_number: 1\n",
      "  split_by_whitespace: 1\n",
      "  split_digits: 0\n",
      "  pretokenization_delimiter: \n",
      "  treat_whitespace_as_suffix: 0\n",
      "  allow_whitespace_only_pieces: 0\n",
      "  required_chars: \n",
      "  byte_fallback: 0\n",
      "  vocabulary_output_piece_score: 1\n",
      "  train_extremely_large_corpus: 0\n",
      "  seed_sentencepieces_file: \n",
      "  hard_vocab_limit: 1\n",
      "  use_all_vocab: 0\n",
      "  unk_id: 0\n",
      "  bos_id: 1\n",
      "  eos_id: 2\n",
      "  pad_id: -1\n",
      "  unk_piece: <unk>\n",
      "  bos_piece: <s>\n",
      "  eos_piece: </s>\n",
      "  pad_piece: <pad>\n",
      "  unk_surface:  ⁇ \n",
      "  enable_differential_privacy: 0\n",
      "  differential_privacy_noise_level: 0\n",
      "  differential_privacy_clipping_threshold: 0\n",
      "}\n",
      "normalizer_spec {\n",
      "  name: nmt_nfkc\n",
      "  add_dummy_prefix: 1\n",
      "  remove_extra_whitespaces: 1\n",
      "  escape_whitespaces: 1\n",
      "  normalization_rule_tsv: \n",
      "}\n",
      "denormalizer_spec {}\n",
      "trainer_interface.cc(409) LOG(INFO) Loaded all 100001 sentences\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <unk>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: <s>\n",
      "trainer_interface.cc(425) LOG(INFO) Adding meta_piece: </s>\n",
      "trainer_interface.cc(430) LOG(INFO) Normalizing sentences...\n",
      "trainer_interface.cc(539) LOG(INFO) all chars count=13687373\n",
      "trainer_interface.cc(550) LOG(INFO) Done: 99.9512% characters are covered.\n",
      "trainer_interface.cc(560) LOG(INFO) Alphabet size=74\n",
      "trainer_interface.cc(561) LOG(INFO) Final character coverage=0.999512\n",
      "trainer_interface.cc(592) LOG(INFO) Done! preprocessed 100001 sentences.\n",
      "trainer_interface.cc(598) LOG(INFO) Tokenizing input sentences with whitespace: 100001\n",
      "trainer_interface.cc(609) LOG(INFO) Done! 104673\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=289718 min_freq=120\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=84788 size=20 all=3331 active=2053 piece=▁b\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=47422 size=40 all=4518 active=3240 piece=om\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=29022 size=60 all=6139 active=4861 piece=▁g\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=22440 size=80 all=7936 active=6658 piece=▁he\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=16166 size=100 all=9405 active=8127 piece=▁sa\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=16082 min_freq=1310\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=13544 size=120 all=10693 active=2272 piece=▁con\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=11435 size=140 all=12288 active=3867 piece=pp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=9707 size=160 all=13466 active=5045 piece=▁O\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=8573 size=180 all=15132 active=6711 piece=all\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=7550 size=200 all=16972 active=8551 piece=▁or\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=7514 min_freq=1161\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6736 size=220 all=18112 active=2126 piece=▁off\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=6059 size=240 all=19166 active=3180 piece=ok\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=5417 size=260 all=20110 active=4124 piece=▁more\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4896 size=280 all=21272 active=5286 piece=ie\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4496 size=300 all=22493 active=6507 piece=▁cont\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=4470 min_freq=916\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=4164 size=320 all=23441 active=2055 piece=ount\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3982 size=340 all=24610 active=3224 piece=▁It\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3673 size=360 all=25457 active=4071 piece=rit\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3352 size=380 all=26662 active=5276 piece=▁pre\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=3150 size=400 all=27854 active=6468 piece=▁into\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=3145 min_freq=689\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2967 size=420 all=28921 active=2459 piece=▁lik\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2803 size=440 all=29692 active=3230 piece=man\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2556 size=460 all=30640 active=4178 piece=ey\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2459 size=480 all=31727 active=5265 piece=▁government\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2340 size=500 all=32241 active=5779 piece=ob\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=2335 min_freq=546\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2251 size=520 all=33110 active=2342 piece=▁We\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2122 size=540 all=33750 active=2982 piece=▁bl\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=2036 size=560 all=34279 active=3511 piece=▁Wh\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1919 size=580 all=35240 active=4472 piece=other\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1866 size=600 all=35829 active=5061 piece=aking\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1860 min_freq=464\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1786 size=620 all=36394 active=2329 piece=▁str\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1720 size=640 all=36851 active=2786 piece=ful\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1642 size=660 all=37512 active=3447 piece=▁war\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1577 size=680 all=38025 active=3960 piece=▁win\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1542 size=700 all=38884 active=4819 piece=ax\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1542 min_freq=401\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1491 size=720 all=39396 active=2362 piece=min\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1447 size=740 all=39920 active=2886 piece=▁during\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1395 size=760 all=40443 active=3409 piece=imes\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1367 size=780 all=40912 active=3878 piece=ised\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1331 size=800 all=41521 active=4487 piece=▁Mc\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1331 min_freq=357\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1300 size=820 all=42377 active=2913 piece=▁wom\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1262 size=840 all=43115 active=3651 piece=▁camp\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1218 size=860 all=43650 active=4186 piece=cer\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1197 size=880 all=44109 active=4645 piece=▁know\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1165 size=900 all=44615 active=5151 piece=emoc\n",
      "bpe_model_trainer.cc(159) LOG(INFO) Updating active symbols. max_freq=1163 min_freq=312\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1139 size=920 all=44998 active=2607 piece=▁pri\n",
      "bpe_model_trainer.cc(268) LOG(INFO) Added: freq=1110 size=940 all=45470 active=3079 piece=▁attack\n",
      "trainer_interface.cc(687) LOG(INFO) Saving model: data/lm1b_tokenizer.model\n",
      "trainer_interface.cc(699) LOG(INFO) Saving vocabs: data/lm1b_tokenizer.vocab\n"
     ]
    }
   ],
   "source": [
    "# Try to train a tokenizer with a small batch\n",
    "import sentencepiece as spm\n",
    "def sentence_generator():\n",
    "    \"\"\"Generator that yields sentences from the LM1B dataset.\"\"\"\n",
    "    for i, example in enumerate(tfds.as_numpy(ds)):\n",
    "        if i > 100000:\n",
    "            return\n",
    "        # The text field might be bytes, so decode it if needed.\n",
    "        text = example['text']\n",
    "        if isinstance(text, bytes):\n",
    "            text = text.decode('utf-8')\n",
    "        yield text\n",
    "\n",
    "# Train SentencePiece using the sentence iterator.\n",
    "spm.SentencePieceTrainer.train(\n",
    "    model_prefix='data/lm1b_tokenizer',\n",
    "    sentence_iterator=sentence_generator(),  # Use our generator instead of an input file.\n",
    "    vocab_size=1024,\n",
    "    character_coverage=0.9995,   # Adjust character coverage if needed.\n",
    "    model_type='bpe'         # You can also choose 'bpe', 'char', or 'word'.\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 64,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(8, 128)\n"
     ]
    }
   ],
   "source": [
    "sp = spm.SentencePieceProcessor()\n",
    "sp.load('data/lm1b_tokenizer.model')\n",
    "\n",
    "import jax\n",
    "import jax.numpy as jnp\n",
    "import numpy as np\n",
    "\n",
    "def get_encoded_batch():\n",
    "    batched_ds = ds.batch(batch_size)\n",
    "    for batch in tfds.as_numpy(batched_ds):\n",
    "        batch = np.vectorize(lambda x: x.decode('utf-8'))(batch['text'])\n",
    "        batch = [sp.encode(x)[:context_length] for x in batch]\n",
    "        batch = [x + [0] * (context_length - len(x)) for x in batch]\n",
    "        batch = jnp.asarray(batch)\n",
    "        yield batch\n",
    "\n",
    "\n",
    "print(next(iter(get_encoded_batch())).shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import flax.nnx as nnx\n",
    "\n",
    "class LangaugeModel()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
